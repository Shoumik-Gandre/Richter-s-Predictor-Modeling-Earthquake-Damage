{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Light Gradient Boosting Machine for predictions\n",
    "This Notebook attempts to create the best possible lightgbm classifier for the earthquake dataset.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "import optuna\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler, FunctionTransformer\n",
    "from category_encoders import LeaveOneOutEncoder, TargetEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassNamePrefixFeaturesOutMixin\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from os import PathLike\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_FEATURES_PATH=\"D:/ml_competitions/gorkha_earthquake/data/raw/train_values.csv\"\n",
    "TRAINING_LABELS_PATH=\"D:/ml_competitions/gorkha_earthquake/data/raw/train_labels.csv\"\n",
    "TEST_FEATURES_PATH=\"D:/ml_competitions/gorkha_earthquake/data/raw/test_values.csv\"\n",
    "SUBMISSION_FORMAT_PATH=\"D:/ml_competitions/gorkha_earthquake/data/raw/submission_format.csv\"\n",
    "\n",
    "SUBMISSION_DIR=\"D:/ml_competitions/gorkha_earthquake/submissions\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df         = pd.read_csv(TRAINING_FEATURES_PATH,   index_col=0)\n",
    "labels_df           = pd.read_csv(TRAINING_LABELS_PATH,     index_col=0) - 1\n",
    "test_features_df    = pd.read_csv(TEST_FEATURES_PATH,       index_col=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "geo_level_columns = ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id']\n",
    "numerical_columns = ['count_floors_pre_eq', 'age', 'area_percentage', 'height_percentage', 'count_families']\n",
    "categorical_columns = ['foundation_type', 'ground_floor_type', 'land_surface_condition', \n",
    "                       'legal_ownership_status', 'other_floor_type',\n",
    "                       'plan_configuration', 'position', 'roof_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DREncoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 latent_dim: int=16, \n",
    "                 geo_lv1_size: int=31, \n",
    "                 geo_lv2_size: int=1428,\n",
    "                 geo_lv3_size: int=12568) -> None:\n",
    "        super().__init__()\n",
    "        self.geo_lv1_embedder = torch.nn.Embedding(geo_lv1_size, 16)\n",
    "        self.geo_lv2_embedder = torch.nn.Embedding(geo_lv2_size, 128)\n",
    "        self.geo_lv3_embedder = torch.nn.Embedding(geo_lv3_size, 128) \n",
    "        self.compressor = torch.nn.Linear(16+128+128, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_1 = self.geo_lv1_embedder(x[:, 0])\n",
    "        x_2 = self.geo_lv2_embedder(x[:, 1])\n",
    "        x_3 = self.geo_lv3_embedder(x[:, 2])\n",
    "        x = torch.concat((x_1, x_2, x_3), dim=1)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        return self.compressor(x)\n",
    "\n",
    "\n",
    "class GeoDimensionReduction(BaseEstimator, TransformerMixin, ClassNamePrefixFeaturesOutMixin):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            path: PathLike,\n",
    "            latent_dim: int=16, \n",
    "            geo_lv1_size: int=31,\n",
    "            geo_lv2_size: int=1418,\n",
    "            geo_lv3_size: int=11861) -> None:\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.model = DREncoder(\n",
    "            latent_dim, \n",
    "            geo_lv1_size,\n",
    "            geo_lv2_size,\n",
    "            geo_lv3_size\n",
    "        )\n",
    "        self.latent_dim = latent_dim\n",
    "        self.geo_lv1_size = geo_lv1_size\n",
    "        self.geo_lv2_size = geo_lv2_size\n",
    "        self.geo_lv3_size = geo_lv3_size\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y=None, *args, **kwargs):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame, y=None, *args, **kwargs):\n",
    "        # Convert pd to numpy\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values # type: ignore\n",
    "        # Apply encoder\n",
    "        self.model.eval()\n",
    "        X = torch.from_numpy(X).type(torch.long) # type: ignore\n",
    "        return self.model(X).detach().numpy()\n",
    "    \n",
    "class RollUpGeoLv3Encoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 latent_dim: int=16, \n",
    "                 geo_lv3_size: int=11861) -> None:\n",
    "        super().__init__()\n",
    "        self.geo_lv3_embedder = torch.nn.Embedding(geo_lv3_size, 128)\n",
    "        self.compressor = torch.nn.Linear(128, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.geo_lv3_embedder(x).squeeze(1)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        return self.compressor(x)\n",
    "\n",
    "\n",
    "class GeoLv3Rollup(BaseEstimator, TransformerMixin, ClassNamePrefixFeaturesOutMixin):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            path: PathLike,\n",
    "            latent_dim: int=16, \n",
    "            geo_lv3_size: int=11861) -> None:\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.model = RollUpGeoLv3Encoder(\n",
    "            latent_dim, \n",
    "            geo_lv3_size\n",
    "        )\n",
    "        self.latent_dim = latent_dim\n",
    "        self.geo_lv3_size = geo_lv3_size\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y=None, *args, **kwargs):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame, y=None, *args, **kwargs):\n",
    "        # Convert pd to numpy\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values # type: ignore\n",
    "        # Apply encoder\n",
    "        self.model.eval()\n",
    "        X = torch.from_numpy(X).type(torch.long) # type: ignore\n",
    "        return self.model(X).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load All Label Encoders\n",
    "with open(Path.cwd().parent / 'models' / 'geo-lv-1-label-encoder.pickle', 'rb') as f:\n",
    "    le1 = pickle.load(f)\n",
    "with open(Path.cwd().parent / 'models' / 'geo-lv-2-label-encoder.pickle', 'rb') as f:\n",
    "    le2 = pickle.load(f)\n",
    "with open(Path.cwd().parent / 'models' / 'geo-lv-3-label-encoder.pickle', 'rb') as f:\n",
    "    le3 = pickle.load(f)\n",
    "\n",
    "# Prepare Transformers\n",
    "geo_lv1_le = FunctionTransformer(\n",
    "    func=lambda x: np.array(le1.transform(x.values.ravel())).reshape(-1, 1),\n",
    "    feature_names_out='one-to-one'\n",
    ")\n",
    "\n",
    "geo_lv2_le = FunctionTransformer(\n",
    "    func=lambda x: np.array(le2.transform(x.values.ravel())).reshape(-1, 1), \n",
    "    feature_names_out='one-to-one'\n",
    ")\n",
    "\n",
    "geo_lv3_le = FunctionTransformer(\n",
    "    func=lambda x: np.array(le3.transform(x.values.ravel())).reshape(-1, 1), \n",
    "    feature_names_out='one-to-one'\n",
    ")\n",
    "\n",
    "# Dim Reducer\n",
    "geo_dim_reduction_preprocessor = ColumnTransformer([\n",
    "        ('geo1_le', geo_lv1_le, ['geo_level_1_id']),\n",
    "        ('geo2_le', geo_lv2_le, ['geo_level_2_id']),\n",
    "        ('geo3_le', geo_lv3_le, ['geo_level_3_id']),\n",
    "    ], \n",
    "    remainder='drop', \n",
    "    verbose_feature_names_out=False\n",
    ").set_output(transform='pandas')\n",
    "\n",
    "geo_dim_reduction_pipe = Pipeline([\n",
    "    ('label_encoder', geo_dim_reduction_preprocessor),\n",
    "    ('embedder', GeoDimensionReduction(path=Path.cwd().parent / 'models' / 'dim-reduction-32', latent_dim=32)),\n",
    "])\n",
    "\n",
    "# Rollup\n",
    "geo3_rollup_preprocessor = ColumnTransformer([\n",
    "        ('geo3_le', geo_lv3_le, ['geo_level_3_id']),\n",
    "    ], \n",
    "    remainder='drop', \n",
    "    verbose_feature_names_out=False,\n",
    ").set_output(transform='pandas')\n",
    "\n",
    "geo_rollup_pipe = Pipeline([\n",
    "    ('label_encoder', geo3_rollup_preprocessor),\n",
    "    ('embedder', GeoLv3Rollup(path=Path.cwd().parent / 'models' / 'geo3-rollup-16')),\n",
    "])\n",
    "\n",
    "preprocessor_lgbm = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # ('bool', FunctionTransformer(lambda x: np.log(1+x), feature_names_out='one-to-one'), numerical_columns),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_columns),\n",
    "        ('geo_dim_reduction', geo_dim_reduction_pipe, ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id']),\n",
    "        ('geo_rollup', geo_rollup_pipe, ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id']),\n",
    "        ('geos', TargetEncoder(cols=geo_level_columns), ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id']),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "# preprocessor_lgbm.set_output(transform='pandas')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[CV] START .....................................................................\n",
      "[CV] END ................................ score: (test=0.742) total time=  11.3s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   11.5s remaining:    0.0s\n",
      "[CV] START .....................................................................\n",
      "[CV] END ................................ score: (test=0.738) total time=  13.9s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   25.5s remaining:    0.0s\n",
      "[CV] START .....................................................................\n",
      "[CV] END ................................ score: (test=0.742) total time=  11.2s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   36.9s remaining:    0.0s\n",
      "[CV] START .....................................................................\n",
      "[CV] END ................................ score: (test=0.741) total time=  12.7s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:   49.9s remaining:    0.0s\n",
      "[CV] START .....................................................................\n",
      "[CV] END ................................ score: (test=0.741) total time=  11.9s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.0min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.0min finished\n",
      "0.74090\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier, OutputCodeClassifier\n",
    "\n",
    "\n",
    "hyperparams = {\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"verbosity\": -1,\n",
    "    # \"boosting_type\": \"dart\",\n",
    "    # 'colsample_bytree': 0.6841610995217867,\n",
    "    # 'min_child_samples': 5,\n",
    "    # 'num_leaves': 255,\n",
    "    # 'reg_alpha': 0.8062643812740887,\n",
    "    # 'reg_lambda': 0.0007637307639175299,\n",
    "    # 'subsample': 0.8490626929837635,\n",
    "    # 'subsample_freq': 5,\n",
    "    # 'n_estimators': 500,\n",
    "}\n",
    "\n",
    "lgbm_clf = LGBMClassifier(force_row_wise=True, **hyperparams)\n",
    "\n",
    "lgbm_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor_lgbm),\n",
    "    ('classifier', lgbm_clf),\n",
    "])\n",
    "\n",
    "\n",
    "results = cross_val_score(\n",
    "    lgbm_pipeline, \n",
    "    features_df, \n",
    "    labels_df.to_numpy().squeeze(), \n",
    "    cv=StratifiedKFold(n_splits=5), \n",
    "    scoring='f1_micro',\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "#     features_df, \n",
    "#     labels_df.values.ravel(), \n",
    "#     stratify=labels_df.values.ravel(),\n",
    "#     random_state=69\n",
    "# )\n",
    "\n",
    "# X_train = preprocessor_lgbm.fit_transform(X_train, y_train)\n",
    "# X_valid = preprocessor_lgbm.transform(X_valid)\n",
    "\n",
    "# lgbm_clf.fit(X_train, y_train)\n",
    "# y_pred_train = lgbm_clf.predict(X_train)\n",
    "# y_pred_valid = lgbm_clf.predict(X_valid)\n",
    "\n",
    "# f1_train = f1_score(y_train, y_pred_train, average='micro')\n",
    "# f1_valid = f1_score(y_valid, y_pred_valid, average='micro')\n",
    "\n",
    "# print(f'{f1_train :.5f}')\n",
    "# print(f'{f1_valid :.5f}')\n",
    "\n",
    "print(f'{results.mean():.5f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.721 - DR  \n",
    "0.73735 - Target + DR  \n",
    "0.72343 - No transform + DR  \n",
    "0.72722 - DR + OHE  \n",
    "0.74009 - No log transform on numeric  \n",
    "0.74071 - No log, GeoDR + GeoRollUp  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FYI: Objective functions can take additional arguments\n",
    "# (https://optuna.readthedocs.io/en/stable/faq.html#objective-func-additional-args).\n",
    "def objective(\n",
    "        trial: optuna.Trial, \n",
    "        X_train: pd.DataFrame, \n",
    "        y_train: np.ndarray, \n",
    "        X_valid: pd.DataFrame, \n",
    "        y_valid: np.ndarray):\n",
    "\n",
    "    hyperparams = {\n",
    "        \"objective\": \"multiclass\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"gbdt\", \"dart\"]),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.4, 1.0),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.4, 1.0),\n",
    "        \"subsample_freq\": trial.suggest_int(\"subsample_freq\", 1, 7),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 2000),\n",
    "        \"learning_rate\": trial.suggest_float(\"subsample\", 1e-8, 1.0, log=True),\n",
    "    }\n",
    "\n",
    "    # Add a callback for pruning.\n",
    "    lgbm_clf = LGBMClassifier(force_row_wise=True, **hyperparams)\n",
    "        \n",
    "    # lgbm_pipeline = Pipeline([\n",
    "    #     ('preprocessor', preprocessor_lgbm),\n",
    "    #     ('scaler', StandardScaler()),\n",
    "    #     ('classifier', lgbm_clf),\n",
    "    # ]).set_output(transform='default')\n",
    "    lgbm_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor_lgbm),\n",
    "        ('classifier', LGBMClassifier(force_row_wise=True, **hyperparams)),\n",
    "    ]).set_output(transform='default')\n",
    "\n",
    "    results = cross_val_score(\n",
    "        lgbm_pipeline, \n",
    "        X, y, cv=StratifiedKFold(n_splits=5), \n",
    "        scoring='f1_micro')\n",
    "    return results.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-06 19:29:20,038]\u001b[0m A new study created in RDB with name: lgbm-study\u001b[0m\n",
      "\u001b[32m[I 2023-05-06 19:31:06,552]\u001b[0m Trial 0 finished with value: 0.7486080225123477 and parameters: {'reg_alpha': 0.0028518221171421787, 'reg_lambda': 0.8801527240793902, 'num_leaves': 219, 'colsample_bytree': 0.5979600947256924, 'subsample': 0.5697442274995929, 'subsample_freq': 2, 'min_child_samples': 44}. Best is trial 0 with value: 0.7486080225123477.\u001b[0m\n",
      "\u001b[32m[I 2023-05-06 19:32:13,840]\u001b[0m Trial 1 finished with value: 0.7462519367420956 and parameters: {'reg_alpha': 6.259343675120804e-05, 'reg_lambda': 7.37497864798079e-05, 'num_leaves': 108, 'colsample_bytree': 0.43927258392987956, 'subsample': 0.42565778179358565, 'subsample_freq': 7, 'min_child_samples': 68}. Best is trial 0 with value: 0.7486080225123477.\u001b[0m\n",
      "\u001b[32m[I 2023-05-06 19:33:08,685]\u001b[0m Trial 2 finished with value: 0.7419695202569179 and parameters: {'reg_alpha': 3.471572961818051e-07, 'reg_lambda': 4.095549636473527e-05, 'num_leaves': 41, 'colsample_bytree': 0.7240862072309977, 'subsample': 0.7139003415280882, 'subsample_freq': 7, 'min_child_samples': 10}. Best is trial 0 with value: 0.7486080225123477.\u001b[0m\n",
      "\u001b[32m[I 2023-05-06 19:35:04,302]\u001b[0m Trial 3 finished with value: 0.7484660391532703 and parameters: {'reg_alpha': 0.00265515997434319, 'reg_lambda': 2.687901741923424e-08, 'num_leaves': 204, 'colsample_bytree': 0.7343002320963248, 'subsample': 0.8097900734000685, 'subsample_freq': 2, 'min_child_samples': 37}. Best is trial 0 with value: 0.7486080225123477.\u001b[0m\n",
      "\u001b[32m[I 2023-05-06 19:35:57,602]\u001b[0m Trial 4 finished with value: 0.7378367656299145 and parameters: {'reg_alpha': 1.1414637100987017, 'reg_lambda': 6.49939320101809e-07, 'num_leaves': 18, 'colsample_bytree': 0.6479216552452859, 'subsample': 0.9076892246119705, 'subsample_freq': 2, 'min_child_samples': 46}. Best is trial 0 with value: 0.7486080225123477.\u001b[0m\n",
      "\u001b[32m[I 2023-05-06 19:38:01,970]\u001b[0m Trial 5 finished with value: 0.7489648886263137 and parameters: {'reg_alpha': 0.129230160191085, 'reg_lambda': 0.25640828133255733, 'num_leaves': 222, 'colsample_bytree': 0.7508757624627765, 'subsample': 0.8029840187097004, 'subsample_freq': 2, 'min_child_samples': 32}. Best is trial 5 with value: 0.7489648886263137.\u001b[0m\n",
      "\u001b[32m[I 2023-05-06 19:39:40,020]\u001b[0m Trial 6 finished with value: 0.7484123199186001 and parameters: {'reg_alpha': 8.631101820948327e-05, 'reg_lambda': 0.9017005830597188, 'num_leaves': 165, 'colsample_bytree': 0.7314235110504264, 'subsample': 0.5363957819891575, 'subsample_freq': 1, 'min_child_samples': 43}. Best is trial 5 with value: 0.7489648886263137.\u001b[0m\n",
      "\u001b[32m[I 2023-05-06 19:41:12,437]\u001b[0m Trial 7 finished with value: 0.7470692688156138 and parameters: {'reg_alpha': 2.4457930928745303e-08, 'reg_lambda': 0.0001039470887652122, 'num_leaves': 104, 'colsample_bytree': 0.579859119666949, 'subsample': 0.9299290336119789, 'subsample_freq': 3, 'min_child_samples': 74}. Best is trial 5 with value: 0.7489648886263137.\u001b[0m\n",
      "\u001b[32m[I 2023-05-06 19:42:39,011]\u001b[0m Trial 8 finished with value: 0.7477139312158733 and parameters: {'reg_alpha': 0.0002616233180038291, 'reg_lambda': 0.006667238296442451, 'num_leaves': 120, 'colsample_bytree': 0.9415780942098938, 'subsample': 0.49630035085354696, 'subsample_freq': 1, 'min_child_samples': 82}. Best is trial 5 with value: 0.7489648886263137.\u001b[0m\n",
      "\u001b[32m[I 2023-05-06 19:43:53,836]\u001b[0m Trial 9 finished with value: 0.7458029687634431 and parameters: {'reg_alpha': 0.7619393915498347, 'reg_lambda': 2.4490191648779303, 'num_leaves': 80, 'colsample_bytree': 0.9827507442360643, 'subsample': 0.6005926508641067, 'subsample_freq': 7, 'min_child_samples': 54}. Best is trial 5 with value: 0.7489648886263137.\u001b[0m\n",
      "\u001b[32m[I 2023-05-06 19:46:22,612]\u001b[0m Trial 10 finished with value: 0.7491222194862321 and parameters: {'reg_alpha': 4.066309428410529, 'reg_lambda': 0.013889834847646181, 'num_leaves': 253, 'colsample_bytree': 0.859767322835951, 'subsample': 0.7674014678302483, 'subsample_freq': 5, 'min_child_samples': 6}. Best is trial 10 with value: 0.7491222194862321.\u001b[0m\n",
      "\u001b[32m[I 2023-05-06 19:48:42,728]\u001b[0m Trial 11 finished with value: 0.748546624115975 and parameters: {'reg_alpha': 8.11674087647166, 'reg_lambda': 0.018570158333408426, 'num_leaves': 242, 'colsample_bytree': 0.8575006638502021, 'subsample': 0.7614385569104729, 'subsample_freq': 5, 'min_child_samples': 6}. Best is trial 10 with value: 0.7491222194862321.\u001b[0m\n",
      "\u001b[32m[I 2023-05-06 19:50:35,550]\u001b[0m Trial 12 finished with value: 0.7477983525936452 and parameters: {'reg_alpha': 0.1086723196316357, 'reg_lambda': 0.030770468979973178, 'num_leaves': 177, 'colsample_bytree': 0.8499340644356569, 'subsample': 0.8229752105843122, 'subsample_freq': 5, 'min_child_samples': 24}. Best is trial 10 with value: 0.7491222194862321.\u001b[0m\n",
      "\u001b[32m[I 2023-05-06 19:52:38,417]\u001b[0m Trial 13 finished with value: 0.7482204541816572 and parameters: {'reg_alpha': 7.8927382275967215, 'reg_lambda': 6.773266718197936, 'num_leaves': 245, 'colsample_bytree': 0.8354304102941186, 'subsample': 0.6559953996170329, 'subsample_freq': 4, 'min_child_samples': 98}. Best is trial 10 with value: 0.7491222194862321.\u001b[0m\n",
      "\u001b[32m[I 2023-05-06 19:54:59,105]\u001b[0m Trial 14 finished with value: 0.748834418267205 and parameters: {'reg_alpha': 0.11431812701256248, 'reg_lambda': 0.0024196444790879736, 'num_leaves': 255, 'colsample_bytree': 0.8019903526328254, 'subsample': 0.9812107406202373, 'subsample_freq': 5, 'min_child_samples': 23}. Best is trial 10 with value: 0.7491222194862321.\u001b[0m\n",
      "\u001b[32m[I 2023-05-06 19:56:45,859]\u001b[0m Trial 15 finished with value: 0.7479480084883655 and parameters: {'reg_alpha': 0.05902514914354372, 'reg_lambda': 0.11051370857807805, 'num_leaves': 165, 'colsample_bytree': 0.918113019330089, 'subsample': 0.6858295046519284, 'subsample_freq': 4, 'min_child_samples': 24}. Best is trial 10 with value: 0.7491222194862321.\u001b[0m\n",
      "\u001b[32m[I 2023-05-06 19:58:51,354]\u001b[0m Trial 16 finished with value: 0.748281853829619 and parameters: {'reg_alpha': 9.651375436173971, 'reg_lambda': 0.19707736711653373, 'num_leaves': 204, 'colsample_bytree': 0.783265018863872, 'subsample': 0.8371395938025876, 'subsample_freq': 6, 'min_child_samples': 18}. Best is trial 10 with value: 0.7491222194862321.\u001b[0m\n",
      "\u001b[32m[I 2023-05-06 20:00:59,609]\u001b[0m Trial 17 finished with value: 0.74830487762087 and parameters: {'reg_alpha': 0.014993320503249705, 'reg_lambda': 0.0007340111012009235, 'num_leaves': 221, 'colsample_bytree': 0.9112193358967766, 'subsample': 0.7632189250914833, 'subsample_freq': 3, 'min_child_samples': 33}. Best is trial 10 with value: 0.7491222194862321.\u001b[0m\n",
      "\u001b[32m[I 2023-05-06 20:02:42,374]\u001b[0m Trial 18 finished with value: 0.7475757904561855 and parameters: {'reg_alpha': 0.40013375553980424, 'reg_lambda': 0.1412593109291844, 'num_leaves': 144, 'colsample_bytree': 0.8774220251999552, 'subsample': 0.644614382013205, 'subsample_freq': 3, 'min_child_samples': 13}. Best is trial 10 with value: 0.7491222194862321.\u001b[0m\n",
      "\u001b[32m[I 2023-05-06 20:04:47,740]\u001b[0m Trial 19 finished with value: 0.7483739505407035 and parameters: {'reg_alpha': 1.6475272149955027, 'reg_lambda': 8.354776341093554, 'num_leaves': 195, 'colsample_bytree': 0.9999084801999141, 'subsample': 0.7270283401896326, 'subsample_freq': 6, 'min_child_samples': 58}. Best is trial 10 with value: 0.7491222194862321.\u001b[0m\n",
      "\u001b[32m[I 2023-05-06 20:06:55,918]\u001b[0m Trial 20 finished with value: 0.7488420948521066 and parameters: {'reg_alpha': 0.019110485614420056, 'reg_lambda': 0.0026814421172151495, 'num_leaves': 227, 'colsample_bytree': 0.7962330905229709, 'subsample': 0.8771941230601399, 'subsample_freq': 4, 'min_child_samples': 33}. Best is trial 10 with value: 0.7491222194862321.\u001b[0m\n",
      "\u001b[32m[I 2023-05-06 20:09:02,481]\u001b[0m Trial 21 finished with value: 0.7491337283633192 and parameters: {'reg_alpha': 0.050042412039533024, 'reg_lambda': 0.0024562699542973916, 'num_leaves': 226, 'colsample_bytree': 0.7891104728462868, 'subsample': 0.8696524681014628, 'subsample_freq': 4, 'min_child_samples': 31}. Best is trial 21 with value: 0.7491337283633192.\u001b[0m\n",
      "\u001b[32m[I 2023-05-06 20:10:55,476]\u001b[0m Trial 22 finished with value: 0.74890349479456 and parameters: {'reg_alpha': 0.271553703344568, 'reg_lambda': 0.031903148998217286, 'num_leaves': 190, 'colsample_bytree': 0.7673591843413803, 'subsample': 0.7741927741522356, 'subsample_freq': 6, 'min_child_samples': 28}. Best is trial 21 with value: 0.7491337283633192.\u001b[0m\n",
      "\u001b[32m[I 2023-05-06 20:13:27,447]\u001b[0m Trial 23 finished with value: 0.7491836128026256 and parameters: {'reg_alpha': 0.8062643812740887, 'reg_lambda': 0.0007637307639175299, 'num_leaves': 255, 'colsample_bytree': 0.6841610995217867, 'subsample': 0.8490626929837635, 'subsample_freq': 5, 'min_child_samples': 5}. Best is trial 23 with value: 0.7491836128026256.\u001b[0m\n",
      "\u001b[33m[W 2023-05-06 20:14:31,577]\u001b[0m Trial 24 failed with parameters: {'reg_alpha': 1.6923368373627437, 'reg_lambda': 0.0006936984933139493, 'num_leaves': 256, 'colsample_bytree': 0.6768940020832972, 'subsample': 0.8580529817872483, 'subsample_freq': 5, 'min_child_samples': 6} because of the following error: KeyboardInterrupt().\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\Atul Gandre\\AppData\\Local\\Temp\\ipykernel_18428\\2671041981.py\", line 10, in <lambda>\n",
      "    lambda trial: objective(trial, X=features_df, y=labels_df.to_numpy().squeeze()),\n",
      "  File \"C:\\Users\\Atul Gandre\\AppData\\Local\\Temp\\ipykernel_18428\\2996892191.py\", line 31, in objective\n",
      "    results = cross_val_score(\n",
      "  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 515, in cross_val_score\n",
      "    cv_results = cross_validate(\n",
      "  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 266, in cross_validate\n",
      "    results = parallel(\n",
      "  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\joblib\\parallel.py\", line 1088, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 708, in _fit_and_score\n",
      "    test_scores = _score(estimator, X_test, y_test, scorer, error_score)\n",
      "  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 115, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 276, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 73, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\pipeline.py\", line 481, in predict\n",
      "    return self.steps[-1][1].predict(Xt, **predict_params)\n",
      "  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\lightgbm\\sklearn.py\", line 984, in predict\n",
      "    result = self.predict_proba(X, raw_score, start_iteration, num_iteration,\n",
      "  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\lightgbm\\sklearn.py\", line 997, in predict_proba\n",
      "    result = super().predict(X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, **kwargs)\n",
      "  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\lightgbm\\sklearn.py\", line 803, in predict\n",
      "    return self._Booster.predict(X, raw_score=raw_score, start_iteration=start_iteration, num_iteration=num_iteration,\n",
      "  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\lightgbm\\basic.py\", line 3538, in predict\n",
      "    return predictor.predict(data, start_iteration, num_iteration,\n",
      "  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\lightgbm\\basic.py\", line 848, in predict\n",
      "    preds, nrow = self.__pred_for_np2d(data, start_iteration, num_iteration, predict_type)\n",
      "  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\lightgbm\\basic.py\", line 938, in __pred_for_np2d\n",
      "    return inner_predict(mat, start_iteration, num_iteration, predict_type)\n",
      "  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\lightgbm\\basic.py\", line 908, in inner_predict\n",
      "    _safe_call(_LIB.LGBM_BoosterPredictForMat(\n",
      "KeyboardInterrupt\n",
      "\u001b[33m[W 2023-05-06 20:14:35,702]\u001b[0m Trial 24 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(\n\u001b[0;32m      2\u001b[0m     study_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlgbm-study\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m      3\u001b[0m     storage\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msqlite:///lgbm-32.db\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m     pruner\u001b[39m=\u001b[39moptuna\u001b[39m.\u001b[39mpruners\u001b[39m.\u001b[39mMedianPruner(n_warmup_steps\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m), \n\u001b[0;32m      7\u001b[0m )\n\u001b[1;32m----> 9\u001b[0m study\u001b[39m.\u001b[39;49moptimize(\n\u001b[0;32m     10\u001b[0m     \u001b[39mlambda\u001b[39;49;00m trial: objective(trial, X\u001b[39m=\u001b[39;49mfeatures_df, y\u001b[39m=\u001b[39;49mlabels_df\u001b[39m.\u001b[39;49mto_numpy()\u001b[39m.\u001b[39;49msqueeze()), \n\u001b[0;32m     11\u001b[0m     n_trials\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "File \u001b[1;32md:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\optuna\\study\\study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    322\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    330\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    332\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \n\u001b[0;32m    334\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m     _optimize(\n\u001b[0;32m    426\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    427\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    428\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    429\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    430\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    431\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[0;32m    432\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    433\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    434\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    435\u001b[0m     )\n",
      "File \u001b[1;32md:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32md:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32md:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32md:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[37], line 10\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m      1\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(\n\u001b[0;32m      2\u001b[0m     study_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlgbm-study\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m      3\u001b[0m     storage\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msqlite:///lgbm-32.db\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m     pruner\u001b[39m=\u001b[39moptuna\u001b[39m.\u001b[39mpruners\u001b[39m.\u001b[39mMedianPruner(n_warmup_steps\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m), \n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m study\u001b[39m.\u001b[39moptimize(\n\u001b[1;32m---> 10\u001b[0m     \u001b[39mlambda\u001b[39;00m trial: objective(trial, X\u001b[39m=\u001b[39;49mfeatures_df, y\u001b[39m=\u001b[39;49mlabels_df\u001b[39m.\u001b[39;49mto_numpy()\u001b[39m.\u001b[39;49msqueeze()), \n\u001b[0;32m     11\u001b[0m     n_trials\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n",
      "Cell \u001b[1;32mIn[36], line 31\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial, X, y)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39m# lgbm_pipeline = Pipeline([\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39m#     ('preprocessor', preprocessor_lgbm),\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m#     ('scaler', StandardScaler()),\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39m#     ('classifier', lgbm_clf),\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39m# ]).set_output(transform='default')\u001b[39;00m\n\u001b[0;32m     26\u001b[0m lgbm_pipeline \u001b[39m=\u001b[39m Pipeline([\n\u001b[0;32m     27\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mpreprocessor\u001b[39m\u001b[39m'\u001b[39m, preprocessor_lgbm),\n\u001b[0;32m     28\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mclassifier\u001b[39m\u001b[39m'\u001b[39m, LGBMClassifier(force_row_wise\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhyperparams)),\n\u001b[0;32m     29\u001b[0m ])\u001b[39m.\u001b[39mset_output(transform\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m results \u001b[39m=\u001b[39m cross_val_score(\n\u001b[0;32m     32\u001b[0m     lgbm_pipeline, \n\u001b[0;32m     33\u001b[0m     X, y, cv\u001b[39m=\u001b[39;49mStratifiedKFold(n_splits\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m), \n\u001b[0;32m     34\u001b[0m     scoring\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mf1_micro\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     35\u001b[0m \u001b[39mreturn\u001b[39;00m results\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[1;32md:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    513\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[1;32m--> 515\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[0;32m    516\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m    517\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    518\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    519\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[0;32m    520\u001b[0m     scoring\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m\"\u001b[39;49m: scorer},\n\u001b[0;32m    521\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[0;32m    522\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    523\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    524\u001b[0m     fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[0;32m    525\u001b[0m     pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[0;32m    526\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    527\u001b[0m )\n\u001b[0;32m    528\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    268\u001b[0m         clone(estimator),\n\u001b[0;32m    269\u001b[0m         X,\n\u001b[0;32m    270\u001b[0m         y,\n\u001b[0;32m    271\u001b[0m         scorers,\n\u001b[0;32m    272\u001b[0m         train,\n\u001b[0;32m    273\u001b[0m         test,\n\u001b[0;32m    274\u001b[0m         verbose,\n\u001b[0;32m    275\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    276\u001b[0m         fit_params,\n\u001b[0;32m    277\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[0;32m    278\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    279\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[0;32m    280\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    281\u001b[0m     )\n\u001b[0;32m    282\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[0;32m    283\u001b[0m )\n\u001b[0;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32md:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32md:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32md:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32md:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32md:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32md:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32md:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32md:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:708\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    705\u001b[0m result[\u001b[39m\"\u001b[39m\u001b[39mfit_error\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    707\u001b[0m fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[1;32m--> 708\u001b[0m test_scores \u001b[39m=\u001b[39m _score(estimator, X_test, y_test, scorer, error_score)\n\u001b[0;32m    709\u001b[0m score_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time \u001b[39m-\u001b[39m fit_time\n\u001b[0;32m    710\u001b[0m \u001b[39mif\u001b[39;00m return_train_score:\n",
      "File \u001b[1;32md:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:767\u001b[0m, in \u001b[0;36m_score\u001b[1;34m(estimator, X_test, y_test, scorer, error_score)\u001b[0m\n\u001b[0;32m    765\u001b[0m         scores \u001b[39m=\u001b[39m scorer(estimator, X_test)\n\u001b[0;32m    766\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 767\u001b[0m         scores \u001b[39m=\u001b[39m scorer(estimator, X_test, y_test)\n\u001b[0;32m    768\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    769\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(scorer, _MultimetricScorer):\n\u001b[0;32m    770\u001b[0m         \u001b[39m# If `_MultimetricScorer` raises exception, the `error_score`\u001b[39;00m\n\u001b[0;32m    771\u001b[0m         \u001b[39m# parameter is equal to \"raise\".\u001b[39;00m\n",
      "File \u001b[1;32md:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\metrics\\_scorer.py:115\u001b[0m, in \u001b[0;36m_MultimetricScorer.__call__\u001b[1;34m(self, estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    114\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(scorer, _BaseScorer):\n\u001b[1;32m--> 115\u001b[0m         score \u001b[39m=\u001b[39m scorer\u001b[39m.\u001b[39m_score(cached_call, estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    116\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m         score \u001b[39m=\u001b[39m scorer(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\metrics\\_scorer.py:276\u001b[0m, in \u001b[0;36m_PredictScorer._score\u001b[1;34m(self, method_caller, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_score\u001b[39m(\u001b[39mself\u001b[39m, method_caller, estimator, X, y_true, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    249\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Evaluate predicted target values for X relative to y_true.\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \n\u001b[0;32m    251\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[39m        Score function applied to prediction of estimator on X.\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m     y_pred \u001b[39m=\u001b[39m method_caller(estimator, \u001b[39m\"\u001b[39;49m\u001b[39mpredict\u001b[39;49m\u001b[39m\"\u001b[39;49m, X)\n\u001b[0;32m    277\u001b[0m     \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    278\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sign \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_score_func(\n\u001b[0;32m    279\u001b[0m             y_true, y_pred, sample_weight\u001b[39m=\u001b[39msample_weight, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_kwargs\n\u001b[0;32m    280\u001b[0m         )\n",
      "File \u001b[1;32md:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\metrics\\_scorer.py:73\u001b[0m, in \u001b[0;36m_cached_call\u001b[1;34m(cache, estimator, method, *args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Call estimator with method and args and kwargs.\"\"\"\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[39mif\u001b[39;00m cache \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(estimator, method)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     75\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m     \u001b[39mreturn\u001b[39;00m cache[method]\n",
      "File \u001b[1;32md:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\pipeline.py:481\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[39mfor\u001b[39;00m _, name, transform \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter(with_final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    480\u001b[0m     Xt \u001b[39m=\u001b[39m transform\u001b[39m.\u001b[39mtransform(Xt)\n\u001b[1;32m--> 481\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mpredict(Xt, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpredict_params)\n",
      "File \u001b[1;32md:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\lightgbm\\sklearn.py:984\u001b[0m, in \u001b[0;36mLGBMClassifier.predict\u001b[1;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, **kwargs)\u001b[0m\n\u001b[0;32m    981\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X, raw_score\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, start_iteration\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, num_iteration\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    982\u001b[0m             pred_leaf\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, pred_contrib\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    983\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 984\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_proba(X, raw_score, start_iteration, num_iteration,\n\u001b[0;32m    985\u001b[0m                                 pred_leaf, pred_contrib, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    986\u001b[0m     \u001b[39mif\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_objective) \u001b[39mor\u001b[39;00m raw_score \u001b[39mor\u001b[39;00m pred_leaf \u001b[39mor\u001b[39;00m pred_contrib:\n\u001b[0;32m    987\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32md:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\lightgbm\\sklearn.py:997\u001b[0m, in \u001b[0;36mLGBMClassifier.predict_proba\u001b[1;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, **kwargs)\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict_proba\u001b[39m(\u001b[39mself\u001b[39m, X, raw_score\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, start_iteration\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, num_iteration\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    995\u001b[0m                   pred_leaf\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, pred_contrib\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    996\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Docstring is set after definition, using a template.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 997\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mpredict(X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    998\u001b[0m     \u001b[39mif\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_objective) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m (raw_score \u001b[39mor\u001b[39;00m pred_leaf \u001b[39mor\u001b[39;00m pred_contrib):\n\u001b[0;32m    999\u001b[0m         _log_warning(\u001b[39m\"\u001b[39m\u001b[39mCannot compute class probabilities or labels \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1000\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mdue to the usage of customized objective function.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1001\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mReturning raw scores instead.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\lightgbm\\sklearn.py:803\u001b[0m, in \u001b[0;36mLGBMModel.predict\u001b[1;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, **kwargs)\u001b[0m\n\u001b[0;32m    799\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_features \u001b[39m!=\u001b[39m n_features:\n\u001b[0;32m    800\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNumber of features of the model must \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    801\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmatch the input. Model n_features_ is \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_features\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    802\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minput n_features is \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 803\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster\u001b[39m.\u001b[39mpredict(X, raw_score\u001b[39m=\u001b[39mraw_score, start_iteration\u001b[39m=\u001b[39mstart_iteration, num_iteration\u001b[39m=\u001b[39mnum_iteration,\n\u001b[0;32m    804\u001b[0m                              pred_leaf\u001b[39m=\u001b[39mpred_leaf, pred_contrib\u001b[39m=\u001b[39mpred_contrib, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\lightgbm\\basic.py:3538\u001b[0m, in \u001b[0;36mBooster.predict\u001b[1;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape, **kwargs)\u001b[0m\n\u001b[0;32m   3536\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3537\u001b[0m         num_iteration \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m-> 3538\u001b[0m \u001b[39mreturn\u001b[39;00m predictor\u001b[39m.\u001b[39;49mpredict(data, start_iteration, num_iteration,\n\u001b[0;32m   3539\u001b[0m                          raw_score, pred_leaf, pred_contrib,\n\u001b[0;32m   3540\u001b[0m                          data_has_header, is_reshape)\n",
      "File \u001b[1;32md:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\lightgbm\\basic.py:848\u001b[0m, in \u001b[0;36m_InnerPredictor.predict\u001b[1;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape)\u001b[0m\n\u001b[0;32m    846\u001b[0m     preds, nrow \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__pred_for_csc(data, start_iteration, num_iteration, predict_type)\n\u001b[0;32m    847\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, np\u001b[39m.\u001b[39mndarray):\n\u001b[1;32m--> 848\u001b[0m     preds, nrow \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__pred_for_np2d(data, start_iteration, num_iteration, predict_type)\n\u001b[0;32m    849\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    850\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\lightgbm\\basic.py:938\u001b[0m, in \u001b[0;36m_InnerPredictor.__pred_for_np2d\u001b[1;34m(self, mat, start_iteration, num_iteration, predict_type)\u001b[0m\n\u001b[0;32m    936\u001b[0m     \u001b[39mreturn\u001b[39;00m preds, nrow\n\u001b[0;32m    937\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 938\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_predict(mat, start_iteration, num_iteration, predict_type)\n",
      "File \u001b[1;32md:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\lightgbm\\basic.py:908\u001b[0m, in \u001b[0;36m_InnerPredictor.__pred_for_np2d.<locals>.inner_predict\u001b[1;34m(mat, start_iteration, num_iteration, predict_type, preds)\u001b[0m\n\u001b[0;32m    906\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mWrong length of pre-allocated predict array\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    907\u001b[0m out_num_preds \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mc_int64(\u001b[39m0\u001b[39m)\n\u001b[1;32m--> 908\u001b[0m _safe_call(_LIB\u001b[39m.\u001b[39;49mLGBM_BoosterPredictForMat(\n\u001b[0;32m    909\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[0;32m    910\u001b[0m     ptr_data,\n\u001b[0;32m    911\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int(type_ptr_data),\n\u001b[0;32m    912\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int32(mat\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]),\n\u001b[0;32m    913\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int32(mat\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m]),\n\u001b[0;32m    914\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int(C_API_IS_ROW_MAJOR),\n\u001b[0;32m    915\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int(predict_type),\n\u001b[0;32m    916\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int(start_iteration),\n\u001b[0;32m    917\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int(num_iteration),\n\u001b[0;32m    918\u001b[0m     c_str(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpred_parameter),\n\u001b[0;32m    919\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(out_num_preds),\n\u001b[0;32m    920\u001b[0m     preds\u001b[39m.\u001b[39;49mctypes\u001b[39m.\u001b[39;49mdata_as(ctypes\u001b[39m.\u001b[39;49mPOINTER(ctypes\u001b[39m.\u001b[39;49mc_double))))\n\u001b[0;32m    921\u001b[0m \u001b[39mif\u001b[39;00m n_preds \u001b[39m!=\u001b[39m out_num_preds\u001b[39m.\u001b[39mvalue:\n\u001b[0;32m    922\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mWrong length for predict results\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "study = optuna.create_study(\n",
    "    study_name='lgbm-study', \n",
    "    storage='sqlite:///lgbm-32.db', \n",
    "    load_if_exists=True,\n",
    "    direction=\"maximize\",\n",
    "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=10), \n",
    ")\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(features_df, labels_df.values.ravel(), stratify=labels_df.values.ravel())\n",
    "\n",
    "study.optimize(\n",
    "    lambda trial: objective(trial, X=features_df, y=labels_df.to_numpy().squeeze()), \n",
    "    n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.7491836128026256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.6841610995217867,\n",
       " 'min_child_samples': 5,\n",
       " 'num_leaves': 255,\n",
       " 'reg_alpha': 0.8062643812740887,\n",
       " 'reg_lambda': 0.0007637307639175299,\n",
       " 'subsample': 0.8490626929837635,\n",
       " 'subsample_freq': 5}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Best score: {study.best_trial.value}\")\n",
    "study.best_trial.params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n",
      "d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n",
      "d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n",
      "d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n",
      "d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\pipeline.py\", line 405, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\lightgbm\\sklearn.py\", line 967, in fit\n    super().fit(X, _y, sample_weight=sample_weight, init_score=init_score, eval_set=valid_sets,\n  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\lightgbm\\sklearn.py\", line 748, in fit\n    self._Booster = train(\n  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\lightgbm\\engine.py\", line 271, in train\n    booster = Booster(params=params, train_set=train_set)\n  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\lightgbm\\basic.py\", line 2605, in __init__\n    train_set.construct()\n  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\lightgbm\\basic.py\", line 1815, in construct\n    self._lazy_init(self.data, label=self.label,\n  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\lightgbm\\basic.py\", line 1538, in _lazy_init\n    self.__init_from_np2d(data, params_str, ref_dataset)\n  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\lightgbm\\basic.py\", line 1659, in __init_from_np2d\n    _safe_call(_LIB.LGBM_DatasetCreateFromMat(\n  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\lightgbm\\basic.py\", line 125, in _safe_call\n    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))\nlightgbm.basic.LightGBMError: Multiclass objective and metrics don't match\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 20\u001b[0m\n\u001b[0;32m      1\u001b[0m hyperparams \u001b[39m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmetric\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mauc\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[39m'\u001b[39m\u001b[39msubsample_freq\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m5\u001b[39m\n\u001b[0;32m     13\u001b[0m }\n\u001b[0;32m     15\u001b[0m h1n1_lgbm_pipeline \u001b[39m=\u001b[39m Pipeline([\n\u001b[0;32m     16\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mpreprocessor\u001b[39m\u001b[39m'\u001b[39m, preprocessor_lgbm),\n\u001b[0;32m     17\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mclassifier\u001b[39m\u001b[39m'\u001b[39m, LGBMClassifier(force_row_wise\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhyperparams)),\n\u001b[0;32m     18\u001b[0m ])\u001b[39m.\u001b[39mset_output(transform\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m results \u001b[39m=\u001b[39m cross_val_score(\n\u001b[0;32m     21\u001b[0m     h1n1_lgbm_pipeline, \n\u001b[0;32m     22\u001b[0m     features_df, \n\u001b[0;32m     23\u001b[0m     labels_df, \n\u001b[0;32m     24\u001b[0m     cv\u001b[39m=\u001b[39;49mStratifiedKFold(n_splits\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m), scoring\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mf1_micro\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     27\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mresults\u001b[39m.\u001b[39mmean()\u001b[39m:\u001b[39;00m\u001b[39m.5f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m) \u001b[39m# 0.87217\u001b[39;00m\n",
      "File \u001b[1;32md:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    513\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[1;32m--> 515\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[0;32m    516\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m    517\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    518\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    519\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[0;32m    520\u001b[0m     scoring\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m\"\u001b[39;49m: scorer},\n\u001b[0;32m    521\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[0;32m    522\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    523\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    524\u001b[0m     fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[0;32m    525\u001b[0m     pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[0;32m    526\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    527\u001b[0m )\n\u001b[0;32m    528\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:285\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[0;32m    266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    268\u001b[0m         clone(estimator),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[39mfor\u001b[39;00m train, test \u001b[39min\u001b[39;00m cv\u001b[39m.\u001b[39msplit(X, y, groups)\n\u001b[0;32m    283\u001b[0m )\n\u001b[1;32m--> 285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[39mif\u001b[39;00m callable(scoring):\n",
      "File \u001b[1;32md:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[39mif\u001b[39;00m num_failed_fits \u001b[39m==\u001b[39m num_fits:\n\u001b[0;32m    361\u001b[0m     all_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAll the \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt is very likely that your model is misconfigured.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou can try to debug the error by setting error_score=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n\u001b[1;32m--> 367\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    369\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     some_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[0;32m    371\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mnum_failed_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed out of a total of \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe score on these train-test partitions for these parameters\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    377\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\sklearn\\pipeline.py\", line 405, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\lightgbm\\sklearn.py\", line 967, in fit\n    super().fit(X, _y, sample_weight=sample_weight, init_score=init_score, eval_set=valid_sets,\n  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\lightgbm\\sklearn.py\", line 748, in fit\n    self._Booster = train(\n  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\lightgbm\\engine.py\", line 271, in train\n    booster = Booster(params=params, train_set=train_set)\n  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\lightgbm\\basic.py\", line 2605, in __init__\n    train_set.construct()\n  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\lightgbm\\basic.py\", line 1815, in construct\n    self._lazy_init(self.data, label=self.label,\n  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\lightgbm\\basic.py\", line 1538, in _lazy_init\n    self.__init_from_np2d(data, params_str, ref_dataset)\n  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\lightgbm\\basic.py\", line 1659, in __init_from_np2d\n    _safe_call(_LIB.LGBM_DatasetCreateFromMat(\n  File \"d:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\lightgbm\\basic.py\", line 125, in _safe_call\n    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))\nlightgbm.basic.LightGBMError: Multiclass objective and metrics don't match\n"
     ]
    }
   ],
   "source": [
    "hyperparams = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"verbose\": -1,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    'colsample_bytree': 0.6841610995217867,\n",
    "    'min_child_samples': 5,\n",
    "    'num_leaves': 255,\n",
    "    'reg_alpha': 0.8062643812740887,\n",
    "    'reg_lambda': 0.0007637307639175299,\n",
    "    'subsample': 0.8490626929837635,\n",
    "    'subsample_freq': 5\n",
    "}\n",
    "    \n",
    "h1n1_lgbm_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor_lgbm),\n",
    "    ('classifier', LGBMClassifier(force_row_wise=True, **hyperparams)),\n",
    "]).set_output(transform='default')\n",
    "\n",
    "results = cross_val_score(\n",
    "    h1n1_lgbm_pipeline, \n",
    "    features_df, \n",
    "    labels_df, \n",
    "    cv=StratifiedKFold(n_splits=5), scoring='f1_micro'\n",
    ")\n",
    "\n",
    "print(f'{results.mean():.5f}') # 0.87217"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1n1_lgbm_pipeline.fit(features_df, labels_df)\n",
    "\n",
    "test_features_df = pd.read_csv(TEST_FEATURES_PATH, index_col=\"respondent_id\")\n",
    "\n",
    "submission_df = pd.read_csv(SUBMISSION_FORMAT_PATH, index_col=\"respondent_id\")\n",
    "\n",
    "np.testing.assert_array_equal(test_features_df.index.values, submission_df.index.values)\n",
    "\n",
    "# Save predictions to submission data frame\n",
    "submission_df[\"h1n1_vaccine\"] = h1n1_lgbm_pipeline.predict_proba(test_features_df)[:, 1]\n",
    "\n",
    "submission_df.to_csv(Path(SUBMISSION_DIR) / 'lgbm-h1n1.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
