{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction Notebook\n",
    "\n",
    "Create Ready-to-use Sklearn Transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "1. [Imports](#imports)\n",
    "1. [Constants](#constants)\n",
    "1. [Geo Level Feature Extraction](#geo-level-feature-extraction)  \n",
    "    1. [Geo Level ID: Dimensionality Reduction](#geo-dim-reduction)  \n",
    "        1. [Model and Dataset Definition](#geo-dim-reduction-model-def)\n",
    "        1. [Training](#geo-dim-reduction-train)\n",
    "    1. [Geo Level ID: Guess Geo 3 Roll up to Geo 1 and 2](#geo3-rollup)\n",
    "        1. [Model and Dataset Definition](#geo-rollup-model-def)\n",
    "        1. [Training](#geo-rollup-train)\n",
    "1. [All Features](#all-feature-extraction)\n",
    "    1. [Simple Autoencoder](#all-feature-ae)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports <a class=\"anchor\" id=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassNamePrefixFeaturesOutMixin\n",
    "from typing import Tuple, Any\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from os import PathLike"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants <a id=\"constants\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace these with your file paths\n",
    "\n",
    "BASE_DIR = Path('d:\\\\', 'ml_competitions', 'gorkha_earthquake')\n",
    "DATA_DIR = BASE_DIR / 'data' / 'raw'\n",
    "MODEL_DIR = BASE_DIR / 'models'\n",
    "SUBMISSION_DIR = BASE_DIR / 'submissions'\n",
    "\n",
    "TRAINING_FEATURES_PATH = DATA_DIR / \"train_values.csv\"\n",
    "TRAINING_LABELS_PATH = DATA_DIR / \"train_labels.csv\"\n",
    "TEST_FEATURES_PATH = DATA_DIR / \"test_values.csv\"\n",
    "SUBMISSION_FORMAT_PATH = DATA_DIR / \"submission_format.csv\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df         = pd.read_csv(TRAINING_FEATURES_PATH,   index_col=0)\n",
    "labels_df           = pd.read_csv(TRAINING_LABELS_PATH,     index_col=0) - 1\n",
    "test_features_df    = pd.read_csv(TEST_FEATURES_PATH,       index_col=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geo Level Feature Extraction <a class=\"anchor\" id=\"geo-level-feature-extraction\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Encode <a id=\"geo-dim-reduction-le\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "df = pd.concat([features_df, test_features_df])\n",
    "\n",
    "le1 = LabelEncoder().fit(df['geo_level_1_id'])\n",
    "le2 = LabelEncoder().fit(df['geo_level_2_id'])\n",
    "le3 = LabelEncoder().fit(df['geo_level_3_id'])\n",
    "\n",
    "eval = le3.transform(test_features_df['geo_level_3_id'])\n",
    "\n",
    "# Save All Label Encoders\n",
    "with open(Path.cwd().parent / 'models' / 'geo-lv-1-label-encoder.pickle', 'wb') as f:\n",
    "    pickle.dump(le1, f)\n",
    "with open(Path.cwd().parent / 'models' / 'geo-lv-2-label-encoder.pickle', 'wb') as f:\n",
    "    pickle.dump(le2, f)\n",
    "with open(Path.cwd().parent / 'models' / 'geo-lv-3-label-encoder.pickle', 'wb') as f:\n",
    "    pickle.dump(le3, f)\n",
    "\n",
    "del le1, le2, le3\n",
    "\n",
    "# Load All Label Encoders\n",
    "with open(Path.cwd().parent / 'models' / 'geo-lv-1-label-encoder.pickle', 'rb') as f:\n",
    "    le1 = pickle.load(f)\n",
    "with open(Path.cwd().parent / 'models' / 'geo-lv-2-label-encoder.pickle', 'rb') as f:\n",
    "    le2 = pickle.load(f)\n",
    "with open(Path.cwd().parent / 'models' / 'geo-lv-3-label-encoder.pickle', 'rb') as f:\n",
    "    le3 = pickle.load(f)\n",
    "\n",
    "# Prepare Transformers\n",
    "geo_lv1_le = FunctionTransformer(\n",
    "    func=lambda x: np.array(le1.transform(x.values.ravel())).reshape(-1, 1),\n",
    "    feature_names_out='one-to-one'\n",
    ")\n",
    "\n",
    "geo_lv2_le = FunctionTransformer(\n",
    "    func=lambda x: np.array(le2.transform(x.values.ravel())).reshape(-1, 1), \n",
    "    feature_names_out='one-to-one'\n",
    ")\n",
    "\n",
    "geo_lv3_le = FunctionTransformer(\n",
    "    func=lambda x: np.array(le3.transform(x.values.ravel())).reshape(-1, 1), \n",
    "    feature_names_out='one-to-one'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration on how to use the above in a column transformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('geo1_le', geo_lv1_le, ['geo_level_1_id']),\n",
    "    ('geo2_le', geo_lv2_le, ['geo_level_2_id']),\n",
    "    ('geo3_le', geo_lv3_le, ['geo_level_3_id']),\n",
    "], remainder='drop', verbose_feature_names_out=False)\n",
    "\n",
    "preprocessor.set_output(transform='pandas')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('prep', preprocessor),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 1418 11861\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([features_df, test_features_df])\n",
    "df: pd.DataFrame = preprocessor.fit_transform(df) # type: ignore\n",
    "\n",
    "# Get max ID in Geo Level 1\n",
    "max_geo_lv1_id = df['geo_level_1_id'].max() + 1\n",
    "# Get max ID in Geo Level 2\n",
    "max_geo_lv2_id = df['geo_level_2_id'].max() + 1\n",
    "# Get max ID in Geo Level 3\n",
    "max_geo_lv3_id = df['geo_level_3_id'].max() + 1\n",
    "\n",
    "print(\n",
    "    max_geo_lv1_id,\n",
    "    max_geo_lv2_id,\n",
    "    max_geo_lv3_id\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geo Level ID: Dimensionality Reduction <a class=\"anchor\" id=\"geo-dim-reduction\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_lv1_counts = df['geo_level_1_id'].value_counts(normalize=True)\n",
    "geo_lv1_weights = np.zeros(max_geo_lv1_id)\n",
    "geo_lv1_weights[geo_lv1_counts.index] = 1.0 / geo_lv1_counts.to_numpy()\n",
    "\n",
    "geo_lv2_counts = df['geo_level_2_id'].value_counts(normalize=True)\n",
    "geo_lv2_weights = np.zeros(max_geo_lv2_id)\n",
    "geo_lv2_weights[geo_lv2_counts.index] = 1.0 / geo_lv2_counts.to_numpy()\n",
    "\n",
    "geo_lv3_counts = df['geo_level_3_id'].value_counts(normalize=True)\n",
    "geo_lv3_weights = np.zeros(max_geo_lv3_id)\n",
    "geo_lv3_weights[geo_lv3_counts.index] = 1.0 / geo_lv3_counts.to_numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Dataset and Model <a id=\"geo-dim-reduction-model-def\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset for Autoencoder training\n",
    "dataset = TensorDataset(\n",
    "    torch.from_numpy(\n",
    "        (\n",
    "            df[['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id']]\n",
    "                .to_numpy()\n",
    "        )\n",
    "    ).type(torch.long),\n",
    "\n",
    "    torch.from_numpy(\n",
    "        (\n",
    "            df[['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id']]\n",
    "                .to_numpy()\n",
    "        )\n",
    "    ).type(torch.long)\n",
    ")\n",
    "\n",
    "\n",
    "class DREncoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 latent_dim: int=16, \n",
    "                 geo_lv1_size: int=31, \n",
    "                 geo_lv2_size: int=1428,\n",
    "                 geo_lv3_size: int=12568) -> None:\n",
    "        super().__init__()\n",
    "        self.geo_lv1_embedder = torch.nn.Embedding(geo_lv1_size, 16)\n",
    "        self.geo_lv2_embedder = torch.nn.Embedding(geo_lv2_size, 512)\n",
    "        self.geo_lv3_embedder = torch.nn.Embedding(geo_lv3_size, 1024) \n",
    "        self.compressor = torch.nn.Linear(16+512+1024, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_1 = self.geo_lv1_embedder(x[:, 0])\n",
    "        x_2 = self.geo_lv2_embedder(x[:, 1])\n",
    "        x_3 = self.geo_lv3_embedder(x[:, 2])\n",
    "        x = torch.concat((x_1, x_2, x_3), dim=1)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        return self.compressor(x)\n",
    "\n",
    "\n",
    "class DRDecoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,                  \n",
    "                 latent_dim: int=16, \n",
    "                 geo_lv1_size: int=31, \n",
    "                 geo_lv2_size: int=1428,\n",
    "                 geo_lv3_size: int=12568) -> None:\n",
    "        super().__init__()\n",
    "        self.geo_lv1_predictor = torch.nn.Linear(latent_dim, geo_lv1_size)\n",
    "        self.geo_lv2_predictor = torch.nn.Linear(latent_dim, geo_lv2_size)\n",
    "        self.geo_lv3_predictor = torch.nn.Linear(latent_dim, geo_lv3_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        x1 = self.geo_lv1_predictor(x)\n",
    "        x2 = self.geo_lv2_predictor(x)\n",
    "        x3 = self.geo_lv3_predictor(x)\n",
    "        return x1, x2, x3\n",
    "\n",
    "\n",
    "class DRAutoEncoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 latent_dim: int=16, \n",
    "                 geo_lv1_size: int=31, \n",
    "                 geo_lv2_size: int=1428,\n",
    "                 geo_lv3_size: int=12568) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = DREncoder(latent_dim, geo_lv1_size, geo_lv2_size, geo_lv3_size)\n",
    "        self.decoder = DRDecoder(latent_dim, geo_lv1_size, geo_lv2_size, geo_lv3_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        x = self.encoder(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x1, x2, x3 = self.decoder(x)\n",
    "        return x1, x2, x3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training <a id=\"geo-dim-reduction-train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 2715/2715 [00:40<00:00, 66.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss = 1.6755006012721176\n",
      "EPOCH 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 2715/2715 [00:40<00:00, 66.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss = 0.18183904730030515\n",
      "EPOCH 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 2715/2715 [00:40<00:00, 66.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss = 0.06992902428110714\n",
      "EPOCH 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 2715/2715 [00:41<00:00, 64.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss = 0.038181437891659294\n",
      "EPOCH 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 2715/2715 [00:43<00:00, 62.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss = 0.023581333787345136\n",
      "EPOCH 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 2715/2715 [00:41<00:00, 65.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss = 0.01687401076191333\n",
      "EPOCH 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 2715/2715 [00:41<00:00, 65.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss = 0.012068605263272393\n",
      "EPOCH 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 2715/2715 [00:41<00:00, 64.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss = 0.009244455388306793\n",
      "EPOCH 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 2715/2715 [00:41<00:00, 65.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss = 0.006961691445714115\n",
      "EPOCH 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 2715/2715 [00:41<00:00, 64.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss = 0.0062905947200633016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "num_epochs = 10\n",
    "dataloader = DataLoader(dataset, 128)\n",
    "model = DRAutoEncoder(\n",
    "    latent_dim=16, \n",
    "    geo_lv1_size=max_geo_lv1_id, \n",
    "    geo_lv2_size=max_geo_lv2_id, \n",
    "    geo_lv3_size=max_geo_lv3_id\n",
    ").to(DEVICE)\n",
    "\n",
    "has_weights = True\n",
    "\n",
    "geo_lv1_weights = compute_class_weight('balanced', classes=df['geo_level_1_id'].unique(), y=df['geo_level_1_id'].values)\n",
    "geo_lv2_weights = compute_class_weight('balanced', classes=df['geo_level_2_id'].unique(), y=df['geo_level_2_id'].values)\n",
    "geo_lv3_weights = compute_class_weight('balanced', classes=df['geo_level_3_id'].unique(), y=df['geo_level_3_id'].values)\n",
    "\n",
    "criterion_geo_lv1 = torch.nn.CrossEntropyLoss(\n",
    "    (\n",
    "        torch\n",
    "            .from_numpy(geo_lv1_weights)\n",
    "            .type(torch.float)\n",
    "            .to(DEVICE)\n",
    "    ) if has_weights else None\n",
    ")\n",
    "criterion_geo_lv2 = torch.nn.CrossEntropyLoss(\n",
    "    (\n",
    "        torch\n",
    "            .from_numpy(geo_lv2_weights)\n",
    "            .type(torch.float)\n",
    "            .to(DEVICE)\n",
    "    ) if has_weights else None\n",
    ")\n",
    "criterion_geo_lv3 = torch.nn.CrossEntropyLoss(\n",
    "    (\n",
    "        torch\n",
    "            .from_numpy(geo_lv3_weights)\n",
    "            .type(torch.float)\n",
    "            .to(DEVICE)\n",
    "    ) if has_weights else None\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"EPOCH {epoch+1}\")\n",
    "    model.train()\n",
    "    training_loss = 0.0\n",
    "    for x, y in tqdm(dataloader, desc=\"training\"):\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        a = model(x)\n",
    "\n",
    "        loss: torch.Tensor = (\n",
    "            criterion_geo_lv1(a[0], y[:, 0])\n",
    "            + criterion_geo_lv2(a[1], y[:, 1])\n",
    "            + criterion_geo_lv3(a[2], y[:, 2])\n",
    "        ) / 3\n",
    "\n",
    "        training_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    training_loss /= len(dataloader)\n",
    "    print(f\"{training_loss = }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.encoder.state_dict(), MODEL_DIR / 'dim-reduction-16-large-w.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DREncoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 latent_dim: int=16, \n",
    "                 geo_lv1_size: int=31, \n",
    "                 geo_lv2_size: int=1418,\n",
    "                 geo_lv3_size: int=11861) -> None:\n",
    "        super().__init__()\n",
    "        self.geo_lv1_embedder = torch.nn.Embedding(geo_lv1_size, 16)\n",
    "        self.geo_lv2_embedder = torch.nn.Embedding(geo_lv2_size, 128)\n",
    "        self.geo_lv3_embedder = torch.nn.Embedding(geo_lv3_size, 128) \n",
    "        self.compressor = torch.nn.Linear(16+128+128, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_1 = self.geo_lv1_embedder(x[:, 0])\n",
    "        x_2 = self.geo_lv2_embedder(x[:, 1])\n",
    "        x_3 = self.geo_lv3_embedder(x[:, 2])\n",
    "        x = torch.concat((x_1, x_2, x_3), dim=1)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        return self.compressor(x)\n",
    "\n",
    "\n",
    "class GeoDimensionReduction(BaseEstimator, TransformerMixin, ClassNamePrefixFeaturesOutMixin):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            path: PathLike,\n",
    "            latent_dim: int=16, \n",
    "            geo_lv1_size: int=31,\n",
    "            geo_lv2_size: int=1418,\n",
    "            geo_lv3_size: int=11861) -> None:\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.model = DREncoder(\n",
    "            latent_dim, \n",
    "            geo_lv1_size,\n",
    "            geo_lv2_size,\n",
    "            geo_lv3_size\n",
    "        )\n",
    "        self.latent_dim = latent_dim\n",
    "        self.geo_lv1_size = geo_lv1_size\n",
    "        self.geo_lv2_size = geo_lv2_size\n",
    "        self.geo_lv3_size = geo_lv3_size\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y=None, *args, **kwargs):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame, y=None, *args, **kwargs):\n",
    "        # Convert pd to numpy\n",
    "        X = X.values\n",
    "        # Apply encoder\n",
    "        self.model.eval()\n",
    "        X = torch.from_numpy(X).type(torch.long) # type: ignore\n",
    "        return self.model(X).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = features_df\n",
    "\n",
    "geo_dim_reduction_pipe = Pipeline([\n",
    "    ('label_encoder', preprocessor),\n",
    "    ('embedder', GeoDimensionReduction(path=Path.cwd().parent / 'models' / 'dim-reduction-16')),\n",
    "])\n",
    "\n",
    "c = ColumnTransformer([\n",
    "    ('geo_dim_reduction', geo_dim_reduction_pipe, ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id'])\n",
    "], remainder='passthrough')\n",
    "c.set_output(transform='pandas')\n",
    "x_ = c.fit_transform(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geo Level ID: Guess Geo 3 Roll up to Geo 1 and 2 <a class=\"anchor\" id=\"geo3-rollup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_lv1_counts = df['geo_level_1_id'].value_counts(normalize=True)\n",
    "geo_lv1_weights = np.zeros(max_geo_lv1_id)\n",
    "geo_lv1_weights[geo_lv1_counts.index] = 1.0 / geo_lv1_counts.to_numpy()\n",
    "\n",
    "geo_lv2_counts = df['geo_level_2_id'].value_counts(normalize=True)\n",
    "geo_lv2_weights = np.zeros(max_geo_lv2_id)\n",
    "geo_lv2_weights[geo_lv2_counts.index] = 1.0 / geo_lv2_counts.to_numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset and Model Definition <a id=\"geo-rollup-model-def\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset for Autoencoder training\n",
    "dataset = TensorDataset(\n",
    "    torch.from_numpy(\n",
    "        (\n",
    "            df[['geo_level_3_id']]\n",
    "                .to_numpy()\n",
    "        )\n",
    "    ).type(torch.long),\n",
    "\n",
    "    torch.from_numpy(\n",
    "        (\n",
    "            df[['geo_level_1_id', 'geo_level_2_id']]\n",
    "                .to_numpy()\n",
    "        )\n",
    "    ).type(torch.long)\n",
    ")\n",
    "\n",
    "\n",
    "class RollUpGeoLv3Encoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 latent_dim: int=16, \n",
    "                 geo_lv3_size: int=11861) -> None:\n",
    "        super().__init__()\n",
    "        self.geo_lv3_embedder = torch.nn.Embedding(geo_lv3_size, 1024)\n",
    "        self.compressor = torch.nn.Linear(1024, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.geo_lv3_embedder(x).squeeze(1)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        return self.compressor(x)\n",
    "\n",
    "\n",
    "class RollUpGeoLv3Decoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,                  \n",
    "                 latent_dim: int=16, \n",
    "                 geo_lv1_size: int=31, \n",
    "                 geo_lv2_size: int=1418) -> None:\n",
    "        super().__init__()\n",
    "        self.geo_lv1_predictor = torch.nn.Linear(latent_dim, geo_lv1_size)\n",
    "        self.geo_lv2_predictor = torch.nn.Linear(latent_dim, geo_lv2_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x1 = self.geo_lv1_predictor(x)\n",
    "        x2 = self.geo_lv2_predictor(x)\n",
    "        return x1, x2\n",
    "\n",
    "\n",
    "class RollUpGeoLv3AutoEncoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 latent_dim: int=16, \n",
    "                 geo_lv1_size: int=31, \n",
    "                 geo_lv2_size: int=1418,\n",
    "                 geo_lv3_size: int=11861) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = RollUpGeoLv3Encoder(latent_dim, geo_lv3_size)\n",
    "        self.decoder = RollUpGeoLv3Decoder(latent_dim, geo_lv1_size, geo_lv2_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.encoder(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x1, x2 = self.decoder(x)\n",
    "        return x1, x2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training <a id=\"geo-rollup-train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 2715/2715 [00:37<00:00, 71.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss = 3.782427275905293\n",
      "EPOCH 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 2715/2715 [00:38<00:00, 71.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss = 1.414766142370512\n",
      "EPOCH 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 2715/2715 [00:37<00:00, 71.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss = 0.6944428952304478\n",
      "EPOCH 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 2715/2715 [00:36<00:00, 74.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss = 0.40136595757825655\n",
      "EPOCH 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 2715/2715 [00:35<00:00, 76.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss = 0.25141762202111645\n",
      "EPOCH 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 2715/2715 [00:35<00:00, 76.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss = 0.16840937440485126\n",
      "EPOCH 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 2715/2715 [00:35<00:00, 76.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss = 0.11941648974440543\n",
      "EPOCH 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 2715/2715 [00:35<00:00, 76.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss = 0.09011306657158284\n",
      "EPOCH 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 2715/2715 [00:35<00:00, 76.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss = 0.06959339500226198\n",
      "EPOCH 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 2715/2715 [00:35<00:00, 76.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss = 0.0541837265434336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "num_epochs = 10\n",
    "dataloader = DataLoader(dataset, 128)\n",
    "model = RollUpGeoLv3AutoEncoder().to(DEVICE)\n",
    "\n",
    "has_weights = True\n",
    "criterion_geo_lv1 = torch.nn.CrossEntropyLoss(\n",
    "    (\n",
    "        torch\n",
    "            .from_numpy(geo_lv1_weights)\n",
    "            .type(torch.float)\n",
    "            .to(DEVICE)\n",
    "    ) if has_weights else None\n",
    ")\n",
    "criterion_geo_lv2 = torch.nn.CrossEntropyLoss(\n",
    "    (\n",
    "        torch\n",
    "            .from_numpy(geo_lv2_weights)\n",
    "            .type(torch.float)\n",
    "            .to(DEVICE)\n",
    "    ) if has_weights else None\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"EPOCH {epoch+1}\")\n",
    "    model.train()\n",
    "    training_loss = 0.0\n",
    "    for x, y in tqdm(dataloader, desc=\"training\"):\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        a = model(x)\n",
    "\n",
    "        loss: torch.Tensor = (\n",
    "            criterion_geo_lv1(a[0], y[:, 0])\n",
    "            + criterion_geo_lv2(a[1], y[:, 1])\n",
    "        ) / 2\n",
    "\n",
    "        training_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    training_loss /= len(dataloader)\n",
    "    print(f\"{training_loss = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.encoder.state_dict(), MODEL_DIR / 'geo3-rollup-16-large-1024')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RollUpGeoLv3Encoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 latent_dim: int=16, \n",
    "                 geo_lv3_size: int=11861) -> None:\n",
    "        super().__init__()\n",
    "        self.geo_lv3_embedder = torch.nn.Embedding(geo_lv3_size, 128)\n",
    "        self.compressor = torch.nn.Linear(128, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.geo_lv3_embedder(x).squeeze(1)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        return self.compressor(x)\n",
    "\n",
    "\n",
    "class GeoLv3Rollup(BaseEstimator, TransformerMixin, ClassNamePrefixFeaturesOutMixin):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            path: PathLike,\n",
    "            latent_dim: int=16, \n",
    "            geo_lv3_size: int=11861) -> None:\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.model = RollUpGeoLv3Encoder(\n",
    "            latent_dim, \n",
    "            geo_lv3_size\n",
    "        )\n",
    "        self.latent_dim = latent_dim\n",
    "        self.geo_lv3_size = geo_lv3_size\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y=None, *args, **kwargs):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame, y=None, *args, **kwargs):\n",
    "        # Convert pd to numpy\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values # type: ignore\n",
    "        # Apply encoder\n",
    "        self.model.eval()\n",
    "        X = torch.from_numpy(X).type(torch.long) # type: ignore\n",
    "        return self.model(X).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = features_df\n",
    "\n",
    "# Demonstration on how to use the above in a column transformer\n",
    "geo3_rollup_preprocessor = ColumnTransformer([\n",
    "        ('geo3_le', geo_lv3_le, ['geo_level_3_id']),\n",
    "    ], \n",
    "    remainder='drop', \n",
    "    verbose_feature_names_out=False,\n",
    ").set_output(transform='pandas')\n",
    "\n",
    "geo_rollup_pipe = Pipeline([\n",
    "    ('label_encoder', geo3_rollup_preprocessor),\n",
    "    ('embedder', GeoLv3Rollup(path=MODEL_DIR / 'geo3-rollup-16')),\n",
    "])\n",
    "\n",
    "c = ColumnTransformer([\n",
    "    ('geo_rollup', geo_rollup_pipe, ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id'])\n",
    "], remainder='drop')\n",
    "c.set_output(transform='pandas')\n",
    "\n",
    "x_ = c.fit_transform(features_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
