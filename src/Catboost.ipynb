{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1NwF9rMZxGNn"
      },
      "source": [
        "# Catboost for predictions\n",
        "This Notebook attempts to create the best possible catboost classifier for the earthquake dataset.  "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MeN-j-mQxGNp"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKIDCG1dxTbn",
        "outputId": "5f4f3f14-3c9c-4e0f-9195-11fcedd2dac9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjnSo3rfxWIZ",
        "outputId": "92597624-0d56-46ae-f569-78ef6bedcbda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/365.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m358.4/365.7 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.2/81.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.9/212.9 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -q optuna category-encoders catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "t6y986LKxGNp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Hyperparameter Optimization\n",
        "import optuna\n",
        "\n",
        "# Preprocessing\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import make_pipeline, Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler, FunctionTransformer\n",
        "from category_encoders import LeaveOneOutEncoder, TargetEncoder, CatBoostEncoder\n",
        "from sklearn.base import BaseEstimator, TransformerMixin, ClassNamePrefixFeaturesOutMixin\n",
        "\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "from os import PathLike\n",
        "import torch"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "e7g5rTsdxGNq"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XSxhPJCJxGNq"
      },
      "outputs": [],
      "source": [
        "BASE_DIR = Path('drive', 'MyDrive', 'ml-competitions') / 'gorkha_earthquake'\n",
        "DATA_DIR = BASE_DIR / 'data'\n",
        "MODEL_DIR = BASE_DIR / 'models'\n",
        "SUBMISSION_DIR = BASE_DIR / 'submissions'\n",
        "\n",
        "TRAINING_FEATURES_PATH = DATA_DIR / \"train_values.csv\"\n",
        "TRAINING_LABELS_PATH = DATA_DIR / \"train_labels.csv\"\n",
        "TEST_FEATURES_PATH = DATA_DIR / \"test_values.csv\"\n",
        "SUBMISSION_FORMAT_PATH = DATA_DIR / \"submission_format.csv\"\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_uqjWr4LxGNq"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jS1ecJKHxGNq"
      },
      "outputs": [],
      "source": [
        "features_df         = pd.read_csv(TRAINING_FEATURES_PATH,   index_col=0)\n",
        "labels_df           = pd.read_csv(TRAINING_LABELS_PATH,     index_col=0) - 1\n",
        "test_features_df    = pd.read_csv(TEST_FEATURES_PATH,       index_col=0)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nw3qnCDPxGNr"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FcBoaNy6xGNr"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "\n",
        "geo_level_columns = ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id']\n",
        "numerical_columns = ['count_floors_pre_eq', 'age', 'area_percentage', 'height_percentage', 'count_families']\n",
        "categorical_columns = ['foundation_type', 'ground_floor_type', 'land_surface_condition', \n",
        "                       'legal_ownership_status', 'other_floor_type',\n",
        "                       'plan_configuration', 'position', 'roof_type']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yLaRt1JVxGNr"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DREncoder(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, \n",
        "                 latent_dim: int=16, \n",
        "                 geo_lv1_size: int=31, \n",
        "                 geo_lv2_size: int=1428,\n",
        "                 geo_lv3_size: int=12568) -> None:\n",
        "        super().__init__()\n",
        "        self.geo_lv1_embedder = torch.nn.Embedding(geo_lv1_size, 16)\n",
        "        self.geo_lv2_embedder = torch.nn.Embedding(geo_lv2_size, 128)\n",
        "        self.geo_lv3_embedder = torch.nn.Embedding(geo_lv3_size, 128) \n",
        "        self.compressor = torch.nn.Linear(16+128+128, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_1 = self.geo_lv1_embedder(x[:, 0])\n",
        "        x_2 = self.geo_lv2_embedder(x[:, 1])\n",
        "        x_3 = self.geo_lv3_embedder(x[:, 2])\n",
        "        x = torch.concat((x_1, x_2, x_3), dim=1)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "        return self.compressor(x)\n",
        "\n",
        "\n",
        "class GeoDimensionReduction(BaseEstimator, TransformerMixin, ClassNamePrefixFeaturesOutMixin):\n",
        "\n",
        "    def __init__(\n",
        "            self, \n",
        "            path: PathLike,\n",
        "            latent_dim: int=16, \n",
        "            geo_lv1_size: int=31,\n",
        "            geo_lv2_size: int=1418,\n",
        "            geo_lv3_size: int=11861, \n",
        "            device=\"cpu\") -> None:\n",
        "        super().__init__()\n",
        "        self.path = path\n",
        "        self.model = DREncoder(\n",
        "            latent_dim, \n",
        "            geo_lv1_size,\n",
        "            geo_lv2_size,\n",
        "            geo_lv3_size\n",
        "        )\n",
        "        self.latent_dim = latent_dim\n",
        "        self.geo_lv1_size = geo_lv1_size\n",
        "        self.geo_lv2_size = geo_lv2_size\n",
        "        self.geo_lv3_size = geo_lv3_size\n",
        "        self.device = device\n",
        "        self.model.load_state_dict(torch.load(path, map_location=device))\n",
        "\n",
        "    def fit(self, X: pd.DataFrame, y=None, *args, **kwargs):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X: pd.DataFrame, y=None, *args, **kwargs):\n",
        "        # Convert pd to numpy\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X = X.values # type: ignore\n",
        "        # Apply encoder\n",
        "        self.model.eval()\n",
        "        X = torch.from_numpy(X).type(torch.long) # type: ignore\n",
        "        return self.model(X).detach().numpy()\n",
        "    \n",
        "class RollUpGeoLv3Encoder(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, \n",
        "                 latent_dim: int=16, \n",
        "                 geo_lv3_size: int=11861) -> None:\n",
        "        super().__init__()\n",
        "        self.geo_lv3_embedder = torch.nn.Embedding(geo_lv3_size, 128)\n",
        "        self.compressor = torch.nn.Linear(128, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.geo_lv3_embedder(x).squeeze(1)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "        return self.compressor(x)\n",
        "\n",
        "\n",
        "class GeoLv3Rollup(BaseEstimator, TransformerMixin, ClassNamePrefixFeaturesOutMixin):\n",
        "\n",
        "    def __init__(\n",
        "            self, \n",
        "            path: PathLike,\n",
        "            latent_dim: int=16, \n",
        "            geo_lv3_size: int=11861,\n",
        "            device=\"cpu\") -> None:\n",
        "        super().__init__()\n",
        "        self.path = path\n",
        "        self.model = RollUpGeoLv3Encoder(\n",
        "            latent_dim, \n",
        "            geo_lv3_size\n",
        "        )\n",
        "        self.latent_dim = latent_dim\n",
        "        self.geo_lv3_size = geo_lv3_size\n",
        "        self.device = device\n",
        "        self.model.load_state_dict(torch.load(path, map_location=device))\n",
        "\n",
        "    def fit(self, X: pd.DataFrame, y=None, *args, **kwargs):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X: pd.DataFrame, y=None, *args, **kwargs):\n",
        "        # Convert pd to numpy\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X = X.values # type: ignore\n",
        "        # Apply encoder\n",
        "        self.model.eval()\n",
        "        X = torch.from_numpy(X).type(torch.long) # type: ignore\n",
        "        return self.model(X).detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRXrwIyLxGNs",
        "outputId": "9a22eea6-6f8a-4fb4-bb9c-ae4b513458a2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LabelEncoder from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Load All Label Encoders\n",
        "with open(MODEL_DIR / 'geo-lv-1-label-encoder.pickle', 'rb') as f:\n",
        "    le1 = pickle.load(f)\n",
        "with open(MODEL_DIR / 'geo-lv-2-label-encoder.pickle', 'rb') as f:\n",
        "    le2 = pickle.load(f)\n",
        "with open(MODEL_DIR / 'geo-lv-3-label-encoder.pickle', 'rb') as f:\n",
        "    le3 = pickle.load(f)\n",
        "\n",
        "# Prepare Transformers\n",
        "geo_lv1_le = FunctionTransformer(\n",
        "    func=lambda x: np.array(le1.transform(x.values.ravel())).reshape(-1, 1),\n",
        "    feature_names_out='one-to-one'\n",
        ")\n",
        "\n",
        "geo_lv2_le = FunctionTransformer(\n",
        "    func=lambda x: np.array(le2.transform(x.values.ravel())).reshape(-1, 1), \n",
        "    feature_names_out='one-to-one'\n",
        ")\n",
        "\n",
        "geo_lv3_le = FunctionTransformer(\n",
        "    func=lambda x: np.array(le3.transform(x.values.ravel())).reshape(-1, 1), \n",
        "    feature_names_out='one-to-one'\n",
        ")\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Dim Reducer\n",
        "geo_dim_reduction_preprocessor = ColumnTransformer([\n",
        "        ('geo1_le', geo_lv1_le, ['geo_level_1_id']),\n",
        "        ('geo2_le', geo_lv2_le, ['geo_level_2_id']),\n",
        "        ('geo3_le', geo_lv3_le, ['geo_level_3_id']),\n",
        "    ], \n",
        "    remainder='drop', \n",
        "    verbose_feature_names_out=False\n",
        ").set_output(transform='pandas')\n",
        "\n",
        "geo_dim_reduction_pipe = Pipeline([\n",
        "    ('label_encoder', geo_dim_reduction_preprocessor),\n",
        "    ('embedder', GeoDimensionReduction(\n",
        "        path=MODEL_DIR / 'dim-reduction-32', \n",
        "        latent_dim=32, \n",
        "        device=DEVICE)\n",
        "    ),\n",
        "])\n",
        "\n",
        "# Rollup\n",
        "geo3_rollup_preprocessor = ColumnTransformer([\n",
        "        ('geo3_le', geo_lv3_le, ['geo_level_3_id']),\n",
        "    ], \n",
        "    remainder='drop', \n",
        "    verbose_feature_names_out=False,\n",
        ").set_output(transform='pandas')\n",
        "\n",
        "geo_rollup_pipe = Pipeline([\n",
        "    ('label_encoder', geo3_rollup_preprocessor),\n",
        "    ('embedder', GeoLv3Rollup(\n",
        "        path=MODEL_DIR / 'geo3-rollup-16', \n",
        "        device=DEVICE)\n",
        "    ),\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('category', FunctionTransformer(\n",
        "            lambda x: x.astype('category'), \n",
        "            feature_names_out='one-to-one'), \n",
        "         categorical_columns + geo_level_columns\n",
        "        ),\n",
        "        # ('numeric', FunctionTransformer(\n",
        "        #     lambda x: np.log(1+x), \n",
        "        #     feature_names_out='one-to-one'), \n",
        "        #  numerical_columns\n",
        "        # ),\n",
        "        ('geo_dim_reduction', geo_dim_reduction_pipe, geo_level_columns),\n",
        "        ('geo_rollup', geo_rollup_pipe, geo_level_columns),\n",
        "        # ('geos', CatBoostEncoder(cols=geo_level_columns), geo_level_columns),\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "# preprocessor.set_output(transform='pandas')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "W00AiufYxGNs"
      },
      "source": [
        "## Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4H2K5Iq8xGNs"
      },
      "outputs": [],
      "source": [
        "hyperparams = {\n",
        "    'task_type': \"GPU\",\n",
        "    'logging_level': 'Silent',\n",
        "    'random_state': 69,\n",
        "    'cat_features': list(range(len(categorical_columns + geo_level_columns))),\n",
        "}\n",
        "\n",
        "clf = CatBoostClassifier(**hyperparams)\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', clf),\n",
        "])\n",
        "\n",
        "\n",
        "results = cross_val_score(\n",
        "    pipe, \n",
        "    features_df, \n",
        "    labels_df.to_numpy().squeeze(), \n",
        "    cv=StratifiedKFold(n_splits=5), \n",
        "    scoring='f1_micro',\n",
        "    verbose=100,\n",
        ")\n",
        "\n",
        "print(f'{results.mean():.5f}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iWKgvQBaxGNt"
      },
      "source": [
        "Scores:\n",
        "\n",
        "0.74893 - \n",
        "```\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', FunctionTransformer(lambda x: x.astype('category'), feature_names_out='one-to-one'), categorical_columns),\n",
        "        # ('bool', FunctionTransformer(lambda x: np.log(1+x), feature_names_out='one-to-one'), numerical_columns),\n",
        "        ('geo_dim_reduction', geo_dim_reduction_pipe, ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id']),\n",
        "        ('geo_rollup', geo_rollup_pipe, ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id']),\n",
        "        ('geos', CatBoostEncoder(cols=geo_level_columns), ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id']),\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "```\n",
        "0.74918 -\n",
        "```\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', FunctionTransformer(lambda x: x.astype('category'), feature_names_out='one-to-one'), categorical_columns),\n",
        "        ('bool', FunctionTransformer(lambda x: np.log(1+x), feature_names_out='one-to-one'), numerical_columns),\n",
        "        ('geo_dim_reduction', geo_dim_reduction_pipe, ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id']),\n",
        "        ('geo_rollup', geo_rollup_pipe, ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id']),\n",
        "        ('geos', CatBoostEncoder(cols=geo_level_columns), ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id']),\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "```\n",
        "0.74925 -\n",
        "```\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('category', FunctionTransformer(\n",
        "            lambda x: x.astype('category'), \n",
        "            feature_names_out='one-to-one'), \n",
        "         categorical_columns + geo_level_columns\n",
        "        ),\n",
        "        ('numeric', FunctionTransformer(\n",
        "              lambda x: np.log(1+x), \n",
        "              feature_names_out='one-to-one'), \n",
        "          numerical_columns\n",
        "        ),\n",
        "        ('geo_dim_reduction', geo_dim_reduction_pipe, geo_level_columns),\n",
        "        ('geo_rollup', geo_rollup_pipe, geo_level_columns),\n",
        "        # ('geos', CatBoostEncoder(cols=geo_level_columns), geo_level_columns),\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "n3n-vzIJoDl8"
      },
      "outputs": [],
      "source": [
        "from typing import Literal\n",
        "from sklearn.base import clone, BaseEstimator, ClassifierMixin\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "\n",
        "class CVBaggingClassifier(BaseEstimator, ClassifierMixin):\n",
        "  \n",
        "  def __init__(self, estimator: BaseEstimator, cv: KFold, num_classes: int):\n",
        "    self.cv = cv\n",
        "    self.num_classes = num_classes\n",
        "    self.estimator = estimator\n",
        "\n",
        "  def fit(self, X: pd.DataFrame, y: np.ndarray):\n",
        "    self.estimators = [clone(self.estimator) for _ in range(self.cv.n_splits)]\n",
        "    self.weights = np.zeros(self.cv.n_splits)\n",
        "    for i, (train_idx, valid_idx) in enumerate(self.cv.split(X, y)):\n",
        "      X_train = X.iloc[train_idx]\n",
        "      X_valid = X.iloc[valid_idx]\n",
        "      y_train = y[train_idx]\n",
        "      y_valid = y[valid_idx]\n",
        "      self.estimators[i].fit(X_train, y_train)\n",
        "      self.weights[i] = f1_score(y_valid, self.estimators[i].predict(X_valid), average='micro')\n",
        "    self.weights /= self.weights.sum()\n",
        "\n",
        "\n",
        "  def predict_proba(self, X, *_):\n",
        "\n",
        "    y_pred = np.zeros((X.shape[0], self.num_classes))\n",
        "\n",
        "    for estimator, weight in zip(self.estimators, self.weights):\n",
        "      y_pred += estimator.predict_proba(X) * weight\n",
        "    \n",
        "    return y_pred\n",
        "\n",
        "  def predict(self, X, *_):\n",
        "    return self.predict_proba(X).argmax(axis=1)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "P3tyUnGvi_eb"
      },
      "outputs": [],
      "source": [
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('category', FunctionTransformer(\n",
        "            lambda x: x.astype('category'), \n",
        "            feature_names_out='one-to-one'), \n",
        "         categorical_columns + geo_level_columns\n",
        "        ),\n",
        "        ('geo_dim_reduction', geo_dim_reduction_pipe, geo_level_columns),\n",
        "        ('geo_rollup', geo_rollup_pipe, geo_level_columns),\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "hyperparams = {\n",
        "    'task_type': \"GPU\",\n",
        "    'logging_level': 'Silent',\n",
        "    'random_state': 69,\n",
        "    'cat_features': list(range(len(categorical_columns + geo_level_columns))),\n",
        "}\n",
        "\n",
        "clf = CatBoostClassifier(**hyperparams)\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', clf),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMCcAg6YnjPA",
        "outputId": "db426ef8-f75a-49f7-cb26-7825e0a44088"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7533115378121594"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bg_clf = CVBaggingClassifier(\n",
        "    estimator=pipe, \n",
        "    cv=StratifiedKFold(10, shuffle=True, random_state=69), \n",
        "    num_classes=3\n",
        ")\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    features_df, labels_df.values.ravel(), stratify=labels_df.values.ravel())\n",
        "\n",
        "bg_clf.fit(X_train, y_train)\n",
        "f1_score(y_valid, bg_clf.predict(X_valid), average='micro')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "My96qbKlAPSm",
        "outputId": "f1510e23-2db4-4325-9532-9897e05159a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.751454313824807"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "a-T6U8a3-e7S"
      },
      "source": [
        "Equal Weights 0.7512087304876364  \n",
        "F1 Weighted 0.751454313824807  \n",
        "F1 Weighted Stratified K-Fold 0.7533115378121594  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "8rx6IIwnvmGK"
      },
      "outputs": [],
      "source": [
        "from os import PathLike\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def create_submission(predictions, submission_formats_path: PathLike):\n",
        "    submission_format = pd.read_csv(submission_formats_path, index_col=0)\n",
        "    submission = pd.DataFrame(data=predictions, columns=submission_format.columns, index=submission_format.index)\n",
        "    submission['damage_grade'] = submission['damage_grade'].astype(int)\n",
        "    return submission\n",
        "\n",
        "bg_clf = CVBaggingClassifier(\n",
        "    estimator=pipe, \n",
        "    cv=StratifiedKFold(10, shuffle=True, random_state=69), \n",
        "    num_classes=3\n",
        ")\n",
        "bg_clf = CVBaggingClassifier(pipe, cv=KFold(10, shuffle=True, random_state=69), num_classes=3)\n",
        "bg_clf.fit(features_df, labels_df.values.ravel())\n",
        "\n",
        "submission = create_submission(bg_clf.predict(test_features_df) + 1, submission_formats_path=SUBMISSION_FORMAT_PATH)\n",
        "submission.to_csv(SUBMISSION_DIR / \"catboost-DR-32-RU-16-bagged-f1weighted.csv\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fUpq-z76xGNu"
      },
      "source": [
        "## Hyperparameter Optimization"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_FidZJtKxGNu"
      },
      "source": [
        "### Objective Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "7bIjJzPyxGNu"
      },
      "outputs": [],
      "source": [
        "# FYI: Objective functions can take additional arguments\n",
        "# (https://optuna.readthedocs.io/en/stable/faq.html#objective-func-additional-args).\n",
        "def objective(trial: optuna.Trial, X_train: pd.DataFrame, y_train: np.ndarray, X_valid: pd.DataFrame, y_valid: np.ndarray):\n",
        "\n",
        "    hyperparams = {\n",
        "        'task_type': \"GPU\",\n",
        "        'logging_level': 'Silent',\n",
        "        'random_state': 69,\n",
        "        'cat_features': list(range(len(categorical_columns + geo_level_columns))),\n",
        "\n",
        "        # \"colsample_bylevel\": trial.suggest_float(\n",
        "        #     \"colsample_bylevel\", 0.01, 0.1\n",
        "        # ),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.01),\n",
        "        \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
        "        # \"l2_leaf_reg\": trial.suggest_int(\"l2_leaf_reg\", 2, 10),\n",
        "        # \"random_strength\": trial.suggest_float(\"random_strength\", 0, 10),\n",
        "    }\n",
        "    \n",
        "    # Add a callback for pruning.\n",
        "    pruning_callback = optuna.integration.CatBoostPruningCallback(\n",
        "        trial, \"validation\")\n",
        "    \n",
        "    clf = CatBoostClassifier(\n",
        "        **hyperparams, \n",
        "    )\n",
        "\n",
        "    clf.fit(\n",
        "        X_train, \n",
        "        y_train,  \n",
        "        eval_set=[(X_valid, y_valid)],\n",
        "        # callbacks=[pruning_callback],\n",
        "        early_stopping_rounds=50, \n",
        "        verbose=False,\n",
        "    )\n",
        "    results = f1_score(y_valid, clf.predict(X_valid), average='micro')\n",
        "    return float(results)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "74ldLRfUxGNu"
      },
      "source": [
        "### Study"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dr9lVK03xGNu"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "study = optuna.create_study(\n",
        "    study_name='catboost-study', \n",
        "    storage='sqlite:///catboost.db', \n",
        "    load_if_exists=True,\n",
        "    direction=\"maximize\",\n",
        "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=10), \n",
        ")\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(features_df, labels_df.values.ravel(), stratify=labels_df.values.ravel())\n",
        "\n",
        "X_train = preprocessor.fit_transform(X_train, y_train)\n",
        "X_valid = preprocessor.transform(X_valid)\n",
        "study.optimize(\n",
        "    lambda trial: objective(trial, X_train, y_train, X_valid, y_valid), \n",
        "    n_trials=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-wjZv9ExGNv"
      },
      "outputs": [],
      "source": [
        "print(f\"Best score: {study.best_trial.value}\")\n",
        "study.best_trial.params"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YtHIRMrfxGNv"
      },
      "source": [
        "### Evaluate Best Model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bdwK2uWsxGNw"
      },
      "source": [
        "## Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wcf8yG0xGNw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
